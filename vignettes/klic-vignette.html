<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>KLIC</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<p>The R package <code>klic</code> (<strong>K</strong>ernel <strong>L</strong>earning <strong>I</strong>ntegrative <strong>C</strong>lustering) contains a collection of tools for integrative clustering of multiple data types. </p>

<p>The main function in this package is <code>klic</code>. It builds one kernel per dataset and combines them through localised kernel k-means to obtain the final clustering. It also allows to try several different values for the number of clusters both at the individual level and for the final clustering. This is the simplest way to perform Kernel Learning Integrative Clustering. However, users may want to add include other types of kernels (instead of using only those derived from consensus clustering) or try different parameters for consensus clustering and include all the corresponding kernels in the analysis. Therefore, we also make available all the functions needed to build a customised KLIC pipeline, that are:</p>

<ul>
<li><p><code>consensusCluster</code>. This function can be used to perform consensus clustering on one dataset and obtain a co-clustering matrix  (Monti et al. 2003). </p></li>
<li><p><code>spectrumShift</code>. This function takes as input any symmetryc matrix (including co-clustering matrices) and checks whether it is positive semi-definite. If not, the eigenvalues of matrix are shifted by a (small) constant in order to make sure that it is a valid kernel. </p></li>
<li><p><code>lmkkmeans</code>. This is a function implemented by Gonen and Margolin (2014) that performs localised kernel k-means on a set of kernel matrices (such as -appropriately shifted- co-clustering matrices, for example).</p></li>
</ul>

<p>The package also includes  <code>coca</code>, a function that we implemented in order to run the Cluster-Of-Clusters-Analysis of TCGA (2012) and compare it with KLIC. </p>

<p>Other functions included in the package are:</p>

<ul>
<li><code>plotSimilarityMatrix</code>, to plot a similarity matrix with the option to divide the rows of the matrix according to the clusters and to include a vector of labels for each row; </li>
<li><code>kkmeansTrain</code>, another function implemented by Gonen and Margolin (2004) to perform kernel k-means with only one kernel matrix (Girolami, 2002);</li>
<li><code>copheneticCorrelation</code>, to calculate the cophenetic correlation of a similarity matrix.</li>
</ul>

<h1>KLIC</h1>

<p>First, we generate four datasets with the same clustering structure (6 clusters of equal size) and different levels of noise.</p>

<pre><code class="r">## Load R packages
#library(Rmosek)
#library(Matrix)
#library(klic)
#library(MASS)
## Set parameters
#n_variables &lt;- 2
#n_obs_cl &lt;- 50
#n_clusters &lt;- 6
#n_separation_levels &lt;- 3
#Sigma &lt;- diag(n_variables)
#N &lt;- n_obs_cl*n_clusters
#P &lt;- n_variables
## Generate synthetic data
#data &lt;- list()
#for(i in 1:n_separation_levels){
#    data[[i]] &lt;- array(NA, c(N, P))
#    mu = rep(NA, N)
#    for(k in 1:n_clusters){
#      mu = rep(k*(i-1), n_variables)
#      data[[i]][((k-1)*n_obs_cl+1):(k*n_obs_cl),] &lt;- mvrnorm(n = n_obs_cl, mu, Sigma)
#    }
#}

# Load synthetic data
data1 &lt;- as.matrix(read.csv(system.file(&quot;extdata&quot;, &quot;dataset1.csv&quot;, package = &quot;klic&quot;), row.names = 1))
data2 &lt;- as.matrix(read.csv(system.file(&quot;extdata&quot;,&quot;dataset2.csv&quot;, package = &quot;klic&quot;), row.names = 1))
data3 &lt;- as.matrix(read.csv(system.file(&quot;extdata&quot;, &quot;dataset3.csv&quot;, package = &quot;klic&quot;), row.names = 1))
data &lt;- list(data1, data2, data3)
n_datasets &lt;- 3
N &lt;- dim(data[[1]])[1]
</code></pre>

<p>Now we can use the <code>consensusClustering</code> function to compute a consensus matrix for each dataset.</p>

<pre><code class="r">## Compute co-clustering matrices for each dataset
CM &lt;- array(NA, c(N, N, n_datasets))
for(i in 1: n_datasets){
  # Scale the columns to have zero mean and unitary variance
  scaledData &lt;- scale(data[[i]]) 
  # Use consensus clustering to find the consensus matrix of each dataset
  CM[,,i] &lt;- consensusCluster(scaledData, K = 6, B = 50)
}
## Plot consensus matrix of one of the datasets
heatmap(CM[,,3], Colv = NA, Rowv = NA)
</code></pre>

<p><img src="figure/consensus_cluster-1.png" alt="plot of chunk consensus_cluster"></p>

<p>Before using kernel methods, we need to make sure that all the consensus matrices are positive semi-definite.</p>

<pre><code class="r">## Check if consensus matrices are PSD and shift eigenvalues if needed.
for(i in 1: n_datasets){
  CM[,,i] &lt;- spectrumShift(CM[,,i], verbose = FALSE)
}
## Plot updated consensus matrix of one of the datasets
heatmap(CM[,,3], Colv = NA, Rowv = NA)
</code></pre>

<p><img src="figure/spectrum_shift-1.png" alt="plot of chunk spectrum_shift"></p>

<p>Now we can perform localised kernel k-kmeans on the set of consensus matrices using the function <code>lmkkmeans</code> to find a global clustering. This functions also need to be provided with a list of parameters containing the prespecified number of clusters and the maximum number of iterations for the k-means algorihtm.</p>

<pre><code class="r">## Perform localised kernel k-means on the consensus matrices
library(Matrix)
library(Rmosek)
parameters &lt;- list()
parameters$cluster_count &lt;- 6 # set the number of clusters K
parameters$iteration_count &lt;- 100 # set the maximum number of iterations
lmkkm &lt;- lmkkmeansTrain(CM, parameters)
</code></pre>

<p>The output of <code>lmkkmeansTrain</code> contains, among other things, the final cluster labels. To compare two clusterings, such as for example the true clustering (that is known in this case) and the clustering found with KLIC, we suggest to use the <code>adjustedRandIndex</code> function of the R package <code>mclust</code>. An ARI close to 1 indicates a high similarity between the two partitions of the data.</p>

<pre><code class="r">## Compare clustering found with KLIC to the true one
ones &lt;- rep(1, N/parameters$cluster_count)
true_labels &lt;- c(ones, ones*2, ones*3, ones*4, ones*5, ones*6)
library(mclust, verbose = FALSE)
adjustedRandIndex(true_labels, lmkkm$clustering) 
</code></pre>

<pre><code>## [1] 0.8990923
</code></pre>

<p>If the global number of clusters is not known, one can find the weights and clusters for different values of K and then find the one that maximises the silhouette.</p>

<pre><code class="r">## Find the value of k that maximises the silhouette

# Initialise array of kernel matrices 
maxK = 6
KM &lt;- array(0, c(N, N, maxK-1))
clLabels &lt;- array(NA, c(maxK-1, N))

parameters &lt;- list()
parameters$iteration_count &lt;- 100 # set the maximum number of iterations

for(i in 2:maxK){

  # Use kernel k-means with K=i to find weights and cluster labels
  parameters$cluster_count &lt;- i # set the number of clusters K
  lmkkm &lt;- lmkkmeansTrain(CM, parameters)

  # Compute weighted matrix
  for(j in 1:dim(CM)[3]){
    KM[,,i-1] &lt;- KM[,,i-1] + (lmkkm$Theta[,j]%*%t(lmkkm$Theta[,j]))*CM[,,j]
  }

  # Save cluster labels
  clLabels[i-1,] &lt;- lmkkm$clustering 
}

# Find value of K that maximises silhouette
maxSil &lt;- maximiseSilhouette(KM, clLabels, maxK = 6)
maxSil$k
</code></pre>

<pre><code>## [1] 6
</code></pre>

<p>The same can be done simply using the function <code>klic</code></p>

<pre><code class="r">klic &lt;- klic(data, M = n_datasets, individualK = c(6,6,6))
klic$globalK
</code></pre>

<pre><code>## [1] 6
</code></pre>

<h1>COCA</h1>

<p>COCA is an alternative method to do integrative clustering of multiple data types (TGCA, 2012).</p>

<pre><code class="r">## Fill label matrix with clusterings found with the k-means clustering algorithm
n_clusters &lt;- 6
n_datasets &lt;- 3
labelMatrix &lt;- array(NA, c(n_clusters, N, n_datasets))
for(i in 1:n_datasets){
  output &lt;- kmeans(data[[i]], n_clusters)
  for(k in 1:n_clusters){
    labelMatrix[k,,i] &lt;- (output$cluster==k)
  }
}
# Convert label matrix from logic to numeric matrix
labelMatrix &lt;- labelMatrix*1
</code></pre>

<p><code>labelMatrix</code> is now a matrix that contains the binary labels of the data given by the k-means clustering algorithm for each dataset separately.</p>

<pre><code class="r"># Build MOC matrix
MOC = rbind(labelMatrix[,,1],labelMatrix[,,2], labelMatrix[,,3])
# Use COCA to find global clustering
coca &lt;- coca(t(MOC), K = 6, hclustMethod = &quot;average&quot;)
# Compare clustering to the true labels
ari &lt;- adjustedRandIndex(true_labels, coca$clusterLabels)
ari
</code></pre>

<pre><code>## [1] 0.6564358
</code></pre>

<h1>Miscellanea</h1>

<p><code>kkmeansTrain</code> is the analogous of <code>lmkkmeans</code>, for just one dataset at a time.</p>

<pre><code class="r">## Set parameters of the kernel k-means algorithm
parameters &lt;- list()
parameters$cluster_count &lt;- 6
parameters$iteration_count &lt;- 100
## Run kernel k-means
kkm &lt;- kkmeansTrain(CM[,,3], parameters)
## Compare clustering to the true labels
clusterLabels &lt;- kkm$clustering
adjustedRandIndex(true_labels, lmkkm$clustering) 
</code></pre>

<pre><code>## [1] 0.8990923
</code></pre>

<p>We also included <code>copheneticCorrelation</code>, a function that calculates the cophenetic correlation coefficient of a similarity matrix </p>

<pre><code class="r">## Compute cophenetic correlation coefficient for each consensus matrix
cc &lt;- rep(NA, n_datasets)
for(i in 1:n_datasets){
  cc[i] &lt;- copheneticCorrelation(CM[,,i])
}
cc
</code></pre>

<pre><code>## [1] 0.8917787 0.8583489 0.8841204
</code></pre>

<p>There is also a function that can be used to plot similarity matrices, ordered by cluster label, and with an additional side label for each row and to save the output in different formats. It is called <code>plotSimilarityMatrix</code>.</p>

<h1>References</h1>

<p>Cabassi, A. and Kirk, P. D. W. (2018). Multiple kernel learning for integrative consensus clustering. In preparation.</p>

<p>Girolami, M. (2002). Mercer kernel-based clustering in feature space. IEEE Transactions on Neural Networks, 13(3), pp.780-784.</p>

<p>Gonen, M. and Margolin, A. A. (2014). Localized Data Fusion for Kernel k-Means Clustering with Application to Cancer Biology. NIPS, (i), 1–9.</p>

<p>Monti, S. et al. (2003). Consensus Clustering: A Resampling-Based Method for Class Discovery and Visualization of Gene. Machine Learning, 52(i), 91–118.</p>

<p>The Cancer Genome Atlas (2012). Comprehensive molecular portraits of human breast tumours. Nature,
487(7407), 61–70.</p>

</body>

</html>
